@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@article{chenRethinkingAtrousConvolution2017,
  title    = {Rethinking {Atrous} {Convolution} for {Semantic} {Image} {Segmentation}},
  url      = {http://arxiv.org/abs/1706.05587},
  abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
  journal  = {arXiv:1706.05587 [cs]},
  author   = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  month    = dec,
  year     = {2017},
  note     = {arXiv: 1706.05587},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{bozorgpourMultiscaleRegionalAttention2021,
  title      = {Multi-scale {Regional} {Attention} {Deeplab3}+: {Multiple} {Myeloma} {Plasma} {Cells} {Segmentation} in {Microscopic} {Images}},
  shorttitle = {Multi-scale {Regional} {Attention} {Deeplab3}+},
  url        = {http://arxiv.org/abs/2105.06238},
  abstract   = {Multiple myeloma cancer is a type of blood cancer that happens when the growth of abnormal plasma cells becomes out of control in the bone marrow. There are various ways to diagnose multiple myeloma in bone marrow such as complete blood count test (CBC) or counting myeloma plasma cell in aspirate slide images using manual visualization or through image processing technique. In this work, an automatic deep learning method for the detection and segmentation of multiple myeloma plasma cell have been explored. To this end, a two-stage deep learning method is designed. In the first stage, the nucleus detection network is utilized to extract each instance of a cell of interest. The extracted instance is then fed to the multi-scale function to generate a multi-scale representation. The objective of the multi-scale function is to capture the shape variation and reduce the effect of object scale on the cytoplasm segmentation network. The generated scales are then fed into a pyramid of cytoplasm networks to learn the segmentation map in various scales. On top of the cytoplasm segmentation network, we included a scale aggregation function to refine and generate a final prediction. The proposed approach has been evaluated on the SegPC2021 grand-challenge and ranked second on the final test phase among all teams.},
  journal    = {arXiv:2105.06238 [cs, eess]},
  author     = {Bozorgpour, Afshin and Azad, Reza and Showkatian, Eman and Sulaiman, Alaa},
  month      = may,
  year       = {2021},
  note       = {arXiv: 2105.06238},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Machine Learning}
}

@article{shiConvolutionalLSTMNetwork2015,
  title      = {Convolutional {LSTM} {Network}: {A} {Machine} {Learning} {Approach} for {Precipitation} {Nowcasting}},
  shorttitle = {Convolutional {LSTM} {Network}},
  url        = {http://arxiv.org/abs/1506.04214},
  abstract   = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
  journal    = {arXiv:1506.04214 [cs]},
  author     = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
  month      = sep,
  year       = {2015},
  note       = {arXiv: 1506.04214},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{linRealTimeHighResolutionBackground2020a,
  title    = {Real-{Time} {High}-{Resolution} {Background} {Matting}},
  url      = {http://arxiv.org/abs/2012.07810},
  abstract = {We introduce a real-time, high-resolution background replacement technique which operates at 30fps in 4K resolution, and 60fps for HD on a modern GPU. Our technique is based on background matting, where an additional frame of the background is captured and used in recovering the alpha matte and the foreground layer. The main challenge is to compute a high-quality alpha matte, preserving strand-level hair details, while processing high-resolution images in real-time. To achieve this goal, we employ two neural networks; a base network computes a low-resolution result which is refined by a second network operating at high-resolution on selective patches. We introduce two largescale video and image matting datasets: VideoMatte240K and PhotoMatte13K/85. Our approach yields higher quality results compared to the previous state-of-the-art in background matting, while simultaneously yielding a dramatic boost in both speed and resolution.},
  journal  = {arXiv:2012.07810 [cs]},
  author   = {Lin, Shanchuan and Ryabtsev, Andrey and Sengupta, Soumyadip and Curless, Brian and Seitz, Steve and Kemelmacher-Shlizerman, Ira},
  month    = dec,
  year     = {2020},
  note     = {arXiv: 2012.07810
              version: 1},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{kirillovPointRendImageSegmentation2020,
  title      = {{PointRend}: {Image} {Segmentation} as {Rendering}},
  shorttitle = {{PointRend}},
  url        = {http://arxiv.org/abs/1912.08193},
  abstract   = {We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over- and undersampling challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state-of-the-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are over-smoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes, for both instance and semantic segmentation. PointRend's efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches. Code has been made available at https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend.},
  journal    = {arXiv:1912.08193 [cs]},
  author     = {Kirillov, Alexander and Wu, Yuxin and He, Kaiming and Girshick, Ross},
  month      = feb,
  year       = {2020},
  note       = {arXiv: 1912.08193},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{linRobustHighResolutionVideo2021,
  title    = {Robust {High}-{Resolution} {Video} {Matting} with {Temporal} {Guidance}},
  url      = {http://arxiv.org/abs/2108.11515},
  abstract = {We introduce a robust, real-time, high-resolution human video matting method that achieves new state-of-the-art performance. Our method is much lighter than previous approaches and can process 4K at 76 FPS and HD at 104 FPS on an Nvidia GTX 1080Ti GPU. Unlike most existing methods that perform video matting frame-by-frame as independent images, our method uses a recurrent architecture to exploit temporal information in videos and achieves significant improvements in temporal coherence and matting quality. Furthermore, we propose a novel training strategy that enforces our network on both matting and segmentation objectives. This significantly improves our model's robustness. Our method does not require any auxiliary inputs such as a trimap or a pre-captured background image, so it can be widely applied to existing human matting applications.},
  journal  = {arXiv:2108.11515 [cs]},
  author   = {Lin, Shanchuan and Yang, Linjie and Saleemi, Imran and Sengupta, Soumyadip},
  month    = aug,
  year     = {2021},
  note     = {arXiv: 2108.11515},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{Rhemann2009APM,
  title   = {A perceptually motivated online benchmark for image matting},
  author  = {Christoph Rhemann and Carsten Rother and Jue Wang and Margrit Gelautz and Pushmeet Kohli and Pamela Rott},
  journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  year    = {2009},
  pages   = {1826-1833}
}

@inproceedings{Ronneberger2015UNetCN,
  title     = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  author    = {Olaf Ronneberger and Philipp Fischer and Thomas Brox},
  booktitle = {MICCAI},
  year      = {2015}
}

@article{Chen2018DeepLabSI,
  title   = {DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs},
  author  = {Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin P. Murphy and Alan Loddon Yuille},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year    = {2018},
  volume  = {40},
  pages   = {834-848}
}

@inproceedings{imagenet_cvpr09,
  author    = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
  title     = {{ImageNet: A Large-Scale Hierarchical Image Database}},
  booktitle = {CVPR09},
  year      = {2009},
  bibsource = {http://www.image-net.org/papers/imagenet_cvpr09.bib}
}

@article{Loshchilov2017FixingWD,
  title   = {Fixing Weight Decay Regularization in Adam},
  author  = {Ilya Loshchilov and Frank Hutter},
  journal = {ArXiv},
  year    = {2017},
  volume  = {abs/1711.05101}
}