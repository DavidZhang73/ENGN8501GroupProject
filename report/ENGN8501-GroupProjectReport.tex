% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

%\documentclass[review]{cvpr}
\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{bbold}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage[toc,page]{appendix}
\usepackage[font=small,labelfont=bf]{caption}

\usepackage{enumitem}

\usepackage{xcolor}
\usepackage{caption}
% \usepackage{subcaption}
\usepackage{bm}
\usepackage{isomath}
\usepackage{arydshln}
\usepackage{stmaryrd}

% \usepackage{fixltx2e}
\usepackage{dblfloatfix}
\usepackage{pbox}
\usepackage{capt-of}

\usepackage[normalem]{ulem}
\usepackage{multirow}

\usepackage{colortbl}

\usepackage{mathtools}
\usepackage{array}

\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother
\usepackage{pgf}

\newcommand{\xpt}{\edef\f@size{\@xpt}\rm}

\def\ie{\emph{i.e.}}
\def\etc{\emph{etc}}

\usepackage{tikz}

\newcommand{\comment}[1]{}

\usepackage{indentfirst}
\input{definitions.tex}

% Include other packages here, before hyperref.
\usepackage[numbib]{tocbibind}
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{8501} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2021}
%\setcounter{page}{4321} % For final version only

\pagestyle{empty}
%\thispagestyle{empty}

\begin{document}

%%%%%%%%% TITLE
\title{Video Matting with Convolutional LSTM}
\author{\underline{Jiahao Zhang} \\ U6921098 \and Peng Zhang \\ U6921163 \and Hang Zhang \\ U6921112}

\maketitle
\begin{abstract}
    TODO
\end{abstract}


\section{Introduction}

background TODO

motivation TODO

related works TODO

\paragraph{Semantic Segmentation}

\paragraph{Video Matting}

\paragraph{Convolutional LSTM}

Here summaries our main contributions,

\begin{itemize}
    \item To our best knowledge, it is the first time to combinate Convolutional LSTM with Deeplab architect for video matting.
    \item We persent a possibility to apply video matting without background as input.
\end{itemize}

\section{Problem Statement}

Given an image $i$ including one or more human figures (Foreground) $f$, we want to decompose it as

\begin{equation}
    i=\alpha f+(1-\alpha)b
\end{equation}\label{formulationImage}

where $alpha$ is the alpha matte, denoted by a greyscale image.
We then define the video as a sequence of images (frames) $V=[i_1, i_2, \dots, i_n]^T$, therefore the alpha matte $A$ is also a sequence of $\alpha$, so is the foreground $F$ and background $B$.

\begin{equation}
    V=AF+(1-A)B
\end{equation}\label{formulationVideo}

The alpha matte $\alpha$ works like a mask here, which means that during training, what we are actually doing is finding $i=\alpha i+(1-\alpha)i$ with constrains $f=\alpha i$ and $b=(1-\alpha)i$.

We also follow \cite{linRealTimeHighResolutionBackground2020a}, not compute $F$ directly, but compute its residual $F_R=F-V$.

Our network $G$ takes $V$ as input and predicts alpha matte $A'$, foreground residual $F_R'$ and error map $E$ and hidden features $H$. The model is fully convolutional therefore it accepts variant resolutions and aspect ratio.

\section{Methods}

In this section, we will introduce both model structure and loss functions for this project.

\subsection{Model Structure}

The model is inspired from the base part of \cite{linRealTimeHighResolutionBackground2020a}, which can be divided into three parts as shown in Figure \ref{modelConvLSTM}. One of the major differences is the input size. Instead of images with background ($B\times 6\times H \times W$), we use video frames without background ($B\times T\times 3\times H \times W$) where $T$ refers to time. Another is we add ConvLSTM after each Decoder Block to obtain temporal features.

\begin{figure*}[htb]
    \begin{center}
        \includegraphics[width=1\textwidth]{img/modelConvLSTM.pdf}
    \end{center}
    \caption{The architecture of our model.}
    \label{modelConvLSTM}
\end{figure*}

\subsubsection{Encoder}

The Encoder consists of four encoder blocks (EB), each contains a convolutional layer, a batch normalization layer and a ReLU activation layer. This architecture is taken from ResNet50 pretrained on ImageNet. It is by design that after each EB, the size of feature maps halves, and the number of channels increases in the order $[3, 64, 256, 512, 2048]$.

\subsubsection{ASPP}

Atrous Spatial Pyramid Pooling (ASPP) utilizes the fusion of multiple convolutions with different dilation rates to increase receptive field, shrink the size of feature maps, just like pooling. But, meanwhile, it can keep more informative features.

\subsubsection{Decoder}

The decoder can be divided into three decoder blocks (DB) and an output block (OB), each of them starts with upsampling, implemented by 2x bilinear interpolation. Together with output of each EB, the feature map then pass a convolutional layer, a batch normalization layer, a ReLU activation layer and ConvLSTM layer. The output size of each DB doubles and the number of channels decreases in the order  $[768, 384, 128, 51, 37]$. The OB generates the output directly, which contains 37 channels. The first channel is Alpha, which is the matte in greyscale. The next 3 channels are Foreground (residual), it is designed to make the model focus more on the foreground. The next channel is Error, it is the predicted error, which will then be compared with the error between the predicted alpha and the ground truth alpha. The rest channels are set for the refine part of \cite{linRealTimeHighResolutionBackground2020a}, which is not used in this project. But we still remain these channels for easier transfer learning.

\subsubsection{ConvLSTM}

\begin{equation}
    \begin{aligned}
        \mathbf{i}_{\mathbf{t}} & = \operatorname{Sigmoid}\left(\operatorname{Conv}\left(\mathbf{x}_{\mathbf{t}} ; \mathbf{w}_{\mathbf{x i}}\right)+\operatorname{Conv}\left(\mathbf{h}_{\mathbf{t}-\mathbf{1}} ; \mathbf{w}_{\mathbf{h i}}\right)+\mathbf{b}_{\mathbf{i}}\right)    \\
        \mathbf{f}_{\mathbf{t}} & = \operatorname{Sigmoid}\left(\operatorname{Conv}\left(\mathbf{x}_{\mathbf{t}} ; \mathbf{w}_{\mathbf{x f}}\right)+\operatorname{Conv}\left(\mathbf{h}_{\mathbf{t}-\mathbf{1}} ; \mathbf{w}_{\mathbf{h f}}\right)+\mathbf{b}_{\mathbf{f}}\right)    \\
        \mathbf{o}_{\mathbf{t}} & = \operatorname{Sigmoid}\left(\operatorname{Conv}\left(\mathbf{x}_{\mathbf{t}} ; \mathbf{w}_{\mathbf{x o}}\right)+\operatorname{Conv}\left(\mathbf{h}_{\mathbf{t}-\mathbf{1}} ; \mathbf{w}_{\mathbf{h o}}\right)+\mathbf{b}_{\mathbf{o}}\right)    \\
        \mathbf{g}_{\mathbf{t}} & = \operatorname{Tanh} \quad\left(\operatorname{Conv}\left(\mathbf{x}_{\mathbf{t}} ; \mathbf{w}_{\mathbf{x g}}\right)+\operatorname{Conv}\left(\mathbf{h}_{\mathbf{t}-\mathbf{1}} ; \mathbf{w}_{\mathbf{h g}}\right)+\mathbf{b}_{\mathbf{g}}\right) \\
        \mathbf{c}_{\mathbf{t}} & = \mathbf{f}_{\mathbf{t}} \odot \mathbf{c}_{\mathbf{t}-\mathbf{1}}+\mathbf{i}_{\mathbf{t}} \odot \mathbf{g}_{\mathbf{t}}                                                                                                                           \\
        \mathbf{h}_{\mathbf{t}} & = \mathbf{o}_{\mathbf{t}} \odot \operatorname{Tanh}\left(\mathbf{c}_{\mathbf{t}}\right)
    \end{aligned}
\end{equation}\label{convLSTM}

TODO

\subsection{Loss Function}

The loss function is a combianation of three losses.

\begin{equation}
    \mathcal{L}_\alpha=\Vert\alpha-\alpha^\star\Vert_1+\Vert\nabla\alpha-\nabla\alpha^\star\Vert_1
\end{equation}\label{lossAlpha}

The first loss is a L1 loss between ground truth alpha $\alpha^\star$ and predicted alpha $\alpha$, and their gradients (Sobel) defined as $\nabla \alpha$ and $\nabla \alpha^\star$ respectively. The gradients loss is added to learn about the contour of the matte.

\section{Experiments}

TODO

\subsection{Experiment Setup}

\subsubsection{Datasets}

TODO

\subsubsection{Metrics}

TODO

\paragraph{MAD (mean absolute difference)}

TODO

\paragraph{MSE (mean squared error)}

TODO

\paragraph{GRAD (Gradient)}

TODO

\paragraph{CONN (Connectivity)}

TODO

\subsubsection{Implementation}

TODO

\subsection{Experiment Results}

TODO

\begin{table}
    \centering
    \caption{caption}
    \label{result}
    \begin{tabular}{lcccc}
        \toprule
        {}       & MAD              & MSE              & GRAD             & CONN               \\
        \midrule
        original & \textbf{2.04}    & \textbf{0.94}    & \textbf{0.11}    & \textbf{102.40}    \\
        \midrule
        transfer & 8.09             & 5.44             & \underline{6.73} & 406.09             \\
        convLSTM & \underline{5.18} & \underline{3.73} & 7.65             & \underline{259.67} \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Comparing}

TODO

\subsubsection{Ablation Study}

TODO

\section{Conclusion}

Conclusion TODO

\cite{chenRethinkingAtrousConvolution2017}

\bibliographystyle{ieee_fullname}
\bibliography{reference}

\section{Review}

\subsection{Self Reflection}

TODO

\subsection{Confidential Peer Review}

TODO

In doing this project, to the best of my judgement,
I confirm that Jiahao Zhang mainly contributed to TODO,
and his/her overall contribution is about 34\%,
Peng Zhang mainly worked on TODO,
and his/her contribution is about 33\%,
and Hang Zhang was responsible for TODO,
and his/her contribution counts about 33\% of the total project workload.

\end{document}
